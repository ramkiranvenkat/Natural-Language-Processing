# -*- coding: utf-8 -*-
"""DLAssn3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OkqJAtvoFUMxzye0mr8vSSx9DLxtJ81L
"""

import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.ticker as ticker
import urllib
import sys
import os
import zipfile
import pickle

glove_zip_file = "glove.6B.zip"
glove_vectors_file = "glove.6B.50d.txt"

snli_zip_file = "snli_1.0.zip"
snli_dev_file = "snli_1.0_dev.txt"
snli_test_file = "snli_1.0_test.txt"
snli_train_file = "snli_1.0_train.txt"
snli_full_dataset_file = "snli_1.0_train.txt"

from six.moves.urllib.request import urlretrieve
    
if (not os.path.isfile(glove_zip_file) and not os.path.isfile(glove_vectors_file)):
    urlretrieve ("http://nlp.stanford.edu/data/glove.6B.zip", glove_zip_file)

if (not os.path.isfile(snli_zip_file) and not os.path.isfile(snli_dev_file)):
    urlretrieve ("https://nlp.stanford.edu/projects/snli/snli_1.0.zip", snli_zip_file)

def unzip_single_file(zip_file_name, output_file_name):
    """
        If the outFile is already created, don't recreate
        If the outFile does not exist, create it from the zipFile
    """
    if not os.path.isfile(output_file_name):
        with open(output_file_name, 'wb') as out_file:
            with zipfile.ZipFile(zip_file_name) as zipped:
                for info in zipped.infolist():
                    if output_file_name in info.filename:
                        with zipped.open(info) as requested_file:
                            out_file.write(requested_file.read())
                            return

unzip_single_file(glove_zip_file, glove_vectors_file)
unzip_single_file(snli_zip_file, snli_dev_file)
unzip_single_file(snli_zip_file, snli_test_file)
unzip_single_file(snli_zip_file, snli_train_file)

import pandas as pd
data =  pd.read_csv("snli_1.0_train.txt",sep='\t')
dataT =  pd.read_csv("snli_1.0_test.txt",sep='\t')
data.dropna(inplace=True, subset=['sentence2']) # Removes the rows where sentence2 is empty
dataT.dropna(inplace=True, subset=['sentence2']) # Removes the rows where sentence2 is empty
blanks =[]
for i,x,y in data[['sentence1','sentence2']].itertuples():
    if (x.isspace() or y.isspace()):
        blanks.append(i)

if (len(blanks) != 0):
    data.drop(blanks, inplace=True)

blanksT =[]
for i,x,y in dataT[['sentence1','sentence2']].itertuples():
    if (x.isspace() or y.isspace()):
        blanksT.append(i)

if (len(blanksT) != 0):
    dataT.drop(blanksT, inplace=True)
    
gl = data['gold_label'].tolist()
s1 = data['sentence1'].tolist()
s2 = data['sentence2'].tolist()

glT = dataT['gold_label'].tolist()
s1T = dataT['sentence1'].tolist()
s2T = dataT['sentence2'].tolist()

gl = pd.Series(gl)
s1 = pd.Series(s1)
s2 = pd.Series(s2)

glT = pd.Series(glT)
s1T = pd.Series(s1T)
s2T = pd.Series(s2T)

print(type(s1))
# pre processing
import io 

import nltk
import re
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')
from nltk.corpus import stopwords 
from nltk.tokenize import word_tokenize 
from nltk.stem import WordNetLemmatizer
stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()
def pre_proc(doc):
    wt = word_tokenize(doc)
    punc = [re.sub(r'[^\w\s]', '', w) for w in wt if re.sub(r'[^\w\s]', '', w) != '']
    lemm = [lemmatizer.lemmatize(w, pos='v') for w in punc]
    filtered_sentence = [w for w in lemm if not w in stop_words]
    sentenceOut = ''
    for word in filtered_sentence:
        sentenceOut += word+" "
    #print(doc+'-->'+sentenceOut)
    sentenceOut = sentenceOut.strip(" ")
    return sentenceOut, filtered_sentence   

#print(vocab)

corpus = []
sntns1 = []
sntns2 = []
vocab =set()
label = []
for i in range(len(gl)):
    so1, wl1 = pre_proc(s1[i].lower())
    so2, wl2 = pre_proc(s2[i].lower())
    #print(so1,"---",so2,'\n',so1.split(),'---',so2.split())
    if (len(so1)>0 and len(so2)>0):
        corpus.append(so1)
        corpus.append(so2)
        sntns1.append(so1)
        sntns2.append(so2)
        vocab.update(wl1)
        vocab.update(wl2)
        label.append(gl[i])
vocab = list(vocab)

print(len(corpus))
sntns1 = pd.Series(sntns1)
sntns2 = pd.Series(sntns2)
label = pd.Series(label)
print(label,sntns1)

corpusT = []
sntns1T = []
sntns2T = []
labelT = []
for i in range(len(glT)):
    so1T, wl1T = pre_proc(s1T[i].lower())
    so2T, wl2T = pre_proc(s2T[i].lower())
    #print(so1,"---",so2,'\n',so1.split(),'---',so2.split())
    if (len(so1T)>0 and len(so2T)>0):
        corpusT.append(so1T)
        corpusT.append(so2T)
        sntns1T.append(so1T)
        sntns2T.append(so2T)
        labelT.append(glT[i])

print(len(corpusT),type(corpusT))
print(sntns1T,labelT)
sntns1T = pd.Series(sntns1T)
sntns2T = pd.Series(sntns2T)
labelT = pd.Series(labelT)
print(sntns1T,labelT)

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer

cv = CountVectorizer(input='content',
    encoding='utf-8',
    decode_error='strict',
    strip_accents=None,
    lowercase=True,
    preprocessor=None,
    tokenizer=None,
    stop_words=None,
    token_pattern='(?u)\\b\\w\\w+\\b',
    ngram_range=(1, 1),
    analyzer='word',
    max_df=1.0,
    min_df=1,
    max_features=None,
    vocabulary=None,
    binary=False)

cv.fit(pd.Series(corpus))

sent1_vect = cv.transform(sntns1)
sent2_vect = cv.transform(sntns2)
sent1_vect.shape, sent2_vect.shape

test_sent1_vect = cv.transform(sntns1T)
test_sent2_vect = cv.transform(sntns2T)

tfidf_transformer = TfidfTransformer()

sent1_tfidf = tfidf_transformer.fit_transform(sent1_vect)
sent2_tfidf = tfidf_transformer.fit_transform(sent2_vect)

tfidf_length, tfidf_width = (sent1_tfidf.shape[0]), (sent1_tfidf.shape[1])
print((sent2_tfidf.shape[0]), (sent2_tfidf.shape[1]), (sent1_tfidf.shape[0]), (sent1_tfidf.shape[1]))

test_sent1_tfidf = tfidf_transformer.transform(test_sent1_vect)
test_sent2_tfidf = tfidf_transformer.transform(test_sent2_vect)

import scipy
from sklearn import preprocessing
from sklearn.linear_model import LogisticRegression
from scipy.sparse import hstack

#X_train = sent1_tfidf-sent2_tfidf
X_train = hstack([sent1_tfidf, sent2_tfidf])
#X_test = test_sent1_tfidf-test_sent2_tfidf
X_test = hstack([test_sent1_tfidf, test_sent2_tfidf])

X_train = preprocessing.scale(X_train, with_mean=False)
X_test = preprocessing.scale(X_test, with_mean=False)

y_train = label
y_train.unique()

lr_reg = LogisticRegression(penalty='l2',
    dual=False,
    tol=0.001,
    C=1.0,
    fit_intercept=True,
    intercept_scaling=1,
    class_weight=None,
    random_state=None,
    solver='saga',
    max_iter=300,
    multi_class='auto',
    verbose=0,
    warm_start=False,
    n_jobs=None,
    l1_ratio=None)

lr_reg.fit(X_train, y_train)

pkl_filename = "lr_model.pkl"
with open(pkl_filename, 'wb') as file:
    pickle.dump(lr_reg, file)

# Load from file
with open(pkl_filename, 'rb') as file:
    pickle_model = pickle.load(file)
    
predictions = pickle_model.predict(X_test)

#predictions = lr_reg.predict(X_test)
print(predictions.shape,labelT.shape)

print(predictions)

from sklearn import metrics
from sklearn.metrics import confusion_matrix, classification_report

print(metrics.accuracy_score(labelT, predictions))
print(confusion_matrix(labelT, predictions))
print(classification_report(labelT, predictions))