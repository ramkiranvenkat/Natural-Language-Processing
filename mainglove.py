# -*- coding: utf-8 -*-
"""mainglove.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1l1XP4NmGRytT3-l4RxAmXpKVA6m1JzJx
"""

import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.ticker as ticker
import urllib
import sys
import os
import zipfile

glovesize =200
glove_vectors_file = "/tmp/glove.6B." + str(glovesize) + "d.txt"
snli_dev_file = "/tmp/snli_1.0/snli_1.0_dev.txt"
snli_test_file = "/tmp/snli_1.0/snli_1.0_test.txt"
snli_train_file = "/tmp/snli_1.0/snli_1.0_train.txt"

from google.colab import drive
drive.mount('/content/drive')
zip_ref = zipfile.ZipFile("/content/drive/My Drive/snli_1.0.zip", 'r')
zip_ref.extractall("/tmp")
zip_ref.close()

zip_ref = zipfile.ZipFile("/content/drive/My Drive/glove.6B.zip", 'r')
zip_ref.extractall("/tmp")
zip_ref.close()

"""
from six.moves.urllib.request import urlretrieve
   
if (not os.path.isfile(glove_zip_file) and not os.path.isfile(glove_vectors_file)):
  urlretrieve ("https://drive.google.com/open?id=1ixGaHUyHRhCQ60rSvwNzMPJc7aWAWtOt", glove_zip_file)

if (not os.path.isfile(snli_zip_file) and not os.path.isfile(snli_dev_file)):
  urlretrieve ("https://drive.google.com/open?id=1SRiw7N2kl45qNJUWCYmzdIhaIP-udYbn", snli_zip_file)

from six.moves.urllib.request import urlretrieve
    
if (not os.path.isfile(glove_zip_file) and not os.path.isfile(glove_vectors_file)):
  urlretrieve ("http://nlp.stanford.edu/data/glove.6B.zip", glove_zip_file)

if (not os.path.isfile(snli_zip_file) and not os.path.isfile(snli_dev_file)):
  urlretrieve ("https://nlp.stanford.edu/projects/snli/snli_1.0.zip", snli_zip_file)
"""

"""
def unzip_single_file(zip_file_name, output_file_name):
    
    #If the outFile is already created, don't recreate
    #If the outFile does not exist, create it from the zipFile
    if not os.path.isfile(output_file_name):
        with open(output_file_name, 'wb') as out_file:
            with zipfile.ZipFile(zip_file_name) as zipped:
                for info in zipped.infolist():
                    if output_file_name in info.filename:
                        with zipped.open(info) as requested_file:
                            out_file.write(requested_file.read())
                            return

unzip_single_file(glove_zip_file, glove_vectors_file)
unzip_single_file(snli_zip_file, snli_dev_file)
unzip_single_file(snli_zip_file, snli_test_file)
unzip_single_file(snli_zip_file, snli_train_file)
"""

glove_wordmap = {}
with open(glove_vectors_file, "r") as glove:
    for line in glove:
        name, vector = tuple(line.split(" ", 1))
        glove_wordmap[name] = np.fromstring(vector, sep=" ")

import pandas as pd
dataT =  pd.read_csv(snli_test_file,sep='\t')
dataT.dropna(inplace=True, subset=['sentence2']) # Removes the rows where sentence2 is empty


blanksT =[]
for i,x,y in dataT[['sentence1','sentence2']].itertuples():
    if (x.isspace() or y.isspace()):
        blanksT.append(i)

if (len(blanksT) != 0):
    dataT.drop(blanksT, inplace=True)

glT = dataT['gold_label'].tolist()
s1T = dataT['sentence1'].tolist()
s2T = dataT['sentence2'].tolist()

print(len(glT))

import nltk
import re
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')
from nltk.corpus import stopwords 
from nltk.tokenize import word_tokenize 
from nltk.stem import WordNetLemmatizer
stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()
def pre_proc(doc):
    wt = word_tokenize(doc)
    punc = [re.sub(r'[^\w\s]', '', w) for w in wt if re.sub(r'[^\w\s]', '', w) != '']
    lemm = [lemmatizer.lemmatize(w, pos='v') for w in punc]
    filtered_sentence = [w for w in lemm if not w in stop_words]
    return filtered_sentence

concatsize = 40
sntnscT = []
labelT = []
lcT = []
for i in range(len(glT)):
    wl1 = pre_proc(s1T[i].lower())
    wl2 = pre_proc(s2T[i].lower())
    if (len(wl1)>0 and len(wl1)>0):
        if (glT[i] == 'contradiction'):
            labelT.append(np.array([1,0,0,0]))
            lcT.append(0)
        elif (glT[i] == 'entailment'):
            labelT.append(np.array([0,1,0,0]))
            lcT.append(1)
        elif (glT[i] == 'neutral'):
            labelT.append(np.array([0,0,1,0]))
            lcT.append(2)
        else:
            labelT.append(np.array([0,0,0,1]))
            lcT.append(3)
      
        gac = np.zeros((concatsize,glovesize))

        igac = 0
        wc = 0
        for word in wl1:
            if word in glove_wordmap and wc < concatsize/2:
                gac[igac,:] = glove_wordmap[word][0:glovesize]
                igac+=1
                wc+=1
        
        wc = 0
        for word in wl2:
            if word in glove_wordmap and wc < concatsize/2:
                gac[igac,:] = glove_wordmap[word][0:glovesize]
                igac+=1
                wc+=1
        gac = np.array(gac)
        sntnscT.append(gac)

#sntnscT = tf.convert_to_tensor(sntnscT)
#labelT = tf.convert_to_tensor(labelT)
sntnscT = np.array(sntnscT)
labelT = np.array(labelT)
lcT = np.array(lcT)
print(labelT.shape,sntnscT.shape)

cmodel = tf.keras.models.load_model('glove-g200-200-32-4-b128.h5')

cmodel.summary()
cmodel.evaluate(sntnscT,labelT,32)

a = cmodel.predict(sntnscT,32)
oitr = np.argmax(a,axis=1)

print(oitr.shape,type(oitr),lcT)

from sklearn import metrics
from sklearn.metrics import confusion_matrix, classification_report

print(metrics.accuracy_score(lcT, oitr))
print(confusion_matrix(lcT, oitr))
print(classification_report(lcT, oitr))

o2 = open('deep_model.txt','a')
for i in range(oitr.shape[0]):
    if (oitr[i] == 0):
        o2.write("contradiction"+'\n')
        continue
    if (oitr[i] == 1):
        o2.write("entailment"+'\n')
        continue
    if (oitr[i] == 2):
        o2.write("neutral"+'\n')
        continue
    o2.write("-"+'\n')
o2.close()